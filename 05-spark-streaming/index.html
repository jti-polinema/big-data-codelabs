
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Big Data - Spark Streaming</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/claat-public/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid="G-H7507PFEJB"
                  id="05-spark-streaming"
                  title="Big Data - Spark Streaming"
                  environment="web"
                  feedback-link="https://github.com/jti-polinema/big-data-codelabs/issues">
    
      <google-codelab-step label="Pengantar" duration="0">
        <p class="image-container"><img style="width: 311.00px" src="img\\909bf8db075bb3db.png"></p>
<p><strong>Terakhir diperbarui: </strong>15 Mei 2023</p>
<p><strong>Penulis: </strong>Habibie Ed Dien</p>
<h2 is-upgraded>Spark Streaming</h2>
<p>Spark Streaming diperkenalkan ke proyek Apache Spark pada rilis 0.7 pada Februari 2013. Dari fase pengujian alfa, versi stabil, 0.9, dirilis pada Februari 2014. API Python ditambahkan di Spark 1.2 dengan beberapa batasan. Semua rilis baru memperkenalkan banyak fitur baru dan peningkatan kinerja seperti Streaming UI, write-ahead logs (WAL), log dengan sekali tulis di awal, dukungan Kafka langsung, dan dukungan tekanan balik.</p>
<h2 is-upgraded><strong>Apa yang Anda akan pelajari</strong></h2>
<p>Di codelab ini Anda akan mempelajari tentang:</p>
<ul>
<li>Praktik menggunakan Spark Streaming</li>
<li>Praktik menggunakan Stateless dan stateful stream processing</li>
<li>Praktik menggunakan Spark Streaming dengan Kafka dan HBase</li>
</ul>
<aside class="special"><p><strong>Catatan:</strong> Materi ini diadaptasi dari Buku Big Data Analytics, Packt Publishing (2016) oleh Venkat Ankam serta sumber dokumentasi resmi lainnya.</p>
</aside>
<h2 is-upgraded><strong>Apa yang Anda perlu persiapkan</strong></h2>
<ul>
<li>PC atau Laptop dengan spesifikasi minimum RAM 8GB Processor Core i3</li>
<li>Koneksi internet</li>
<li>CMD, putty, atau terminal</li>
</ul>
<h2 is-upgraded><strong>Pengetahuan yang Anda harus dimiliki</strong></h2>
<ul>
<li>Diasumsikan Anda telah berhasil setup CDH di VirtualBox, VM, atau docker dengan Hadoop dan Apache Spark</li>
<li>Diasumsikan Anda telah berhasil upgrade ke Spark v2.0 atau yang lebih baru</li>
<li>Pemrograman dasar Java, Python dan Scala</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Praktik Dasar Spark Streaming" duration="30">
        <p>Untuk memulai spark, silakan akses ke folder spark di mana Anda pernah ekstrak sebelumnya. Misalnya di bawah ini terletak di <code>/home/cloudera</code>, lalu masuk ke folder <code>sbin</code> dan run <code>start-all.sh</code> untuk menjalankan spark daemons. Jika diminta mengisi password, maka isikan <code>cloudera</code>.</p>
<pre><code>cd /home/cloudera/spark-2.0.0-bin-hadoop2.7/sbin
sudo ./start-all.sh</code></pre>
<p>Kemudian jika berhasil running, Anda dapat mengakses di browser CDH atau VM pada alamat <a href="http://quickstart.cloudera:8080/" target="_blank">http://quickstart.cloudera:8080/</a> </p>
<h2 is-upgraded><strong>Memahami aplikasi Spark Streaming</strong></h2>
<p>Silakan masuk ke folder <code>spark-2.0.0-bin-hadoop2.7/examples/src/main/python/streaming</code> yang berisi file-file kode seperti berikut.</p>
<p class="image-container"><img style="width: 624.00px" src="img\\c503ee41d2f62fd9.png"></p>
<p>Anda dapat melihat isi kode pada file <code>network_wordcount.py</code> seperti pada Kode 1 berikut ini. Jika Anda <em>running </em>menggunakan Ubuntu atau docker, dapat menyalin kode berikut untuk praktikum.</p>
<h4 is-upgraded>Kode 1</h4>
<pre><code>from __future__ import print_function

import sys

from pyspark import SparkContext
from pyspark.streaming import StreamingContext

if __name__ == &#34;__main__&#34;:
    if len(sys.argv) != 3:
        print(&#34;Usage: network_wordcount.py &lt;hostname&gt; &lt;port&gt;&#34;, file=sys.stderr)
        exit(-1)
    sc = SparkContext(appName=&#34;PythonStreamingNetworkWordCount&#34;)
    ssc = StreamingContext(sc, 1)

    lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))
    counts = lines.flatMap(lambda line: line.split(&#34; &#34;))\
                  .map(lambda word: (word, 1))\
                  .reduceByKey(lambda a, b: a+b)
    counts.pprint()

    ssc.start()
    ssc.awaitTermination()</code></pre>
<h3 is-upgraded><strong>Metode 1: Mode Stateless Stream Processing</strong></h3>
<p>Buka 2 terminal atau CMD untuk melakukan simulasi stateless stream processing dengan spark streaming.</p>
<h4 is-upgraded>Kode 2: Terminal 1</h4>
<pre><code>nc -lk 9999</code></pre>
<p>Pada terminal kedua, lakukan spark-submit berdasarkan Kode 1 sebelumnya seperti berikut.</p>
<h4 is-upgraded>Kode 3: Terminal 2</h4>
<pre><code>spark-submit --master local[*] network_wordcount.py localhost 9999</code></pre>
<p>Hasilnya akan seperti pada gambar berikut ini.</p>
<p class="image-container"><img style="width: 624.00px" src="img\\f5660ad526dc46ff.png"></p>
<p>Coba ketik Nama dan NIM Anda di terminal 1 bagian nc, lalu tekan Enter. Maka akan tampil di terminal 2.</p>
<aside class="warning"><p><strong>Perhatian:</strong> Untuk mengakhiri sesi terminal tersebut, Anda dapat menekan kombinasi di keyboard Ctrl+C</p>
</aside>
<p>Karena streaming interval setiap 1 detik terlalu cepat sehingga tidak terlihat data yang dikirimkan. Sekarang kita coba ganti interval waktu menjadi per 5 detik dengan mengubah kode 1 pada bagian ini:</p>
<pre><code>ssc = StreamingContext(sc, 5)</code></pre>
<p>Lalu coba submit kembali, lakukan simulasi seperti sebelumnya dengan mengirimkan Nama dan NIM Anda di netcat.</p>
<aside class="special"><p><strong>Catatan:</strong> Sampai sini Anda telah berhasil melakukan simulasi stateless stream processing dengan spark streaming. Silakan bisa di-screenshot hasilnya dan dibuat laporan praktikum ke repository GitHub Anda dengan nama repo atau folder (lampirkan code tersebut) <strong><code>spark-streaming/01_stateless</code></strong></p>
</aside>
<h3 is-upgraded><strong>Metode 2: Mode Stateful Stream Processing</strong></h3>
<p>Sekarang coba untuk file <code>stateful_network_wordcount.py</code> yang isi kodenya seperti berikut ini.</p>
<h4 is-upgraded>Kode 4</h4>
<pre><code>from __future__ import print_function

import sys

from pyspark import SparkContext
from pyspark.streaming import StreamingContext

if __name__ == &#34;__main__&#34;:
    if len(sys.argv) != 3:
        print(&#34;Usage: stateful_network_wordcount.py &lt;hostname&gt; &lt;port&gt;&#34;, file=sys.stderr)
        exit(-1)
    sc = SparkContext(appName=&#34;PythonStreamingStatefulNetworkWordCount&#34;)
    ssc = StreamingContext(sc, 5)
    ssc.checkpoint(&#34;checkpoint&#34;)

    # RDD with initial state (key, value) pairs
    initialStateRDD = sc.parallelize([(u&#39;hello&#39;, 1), (u&#39;world&#39;, 1)])

    def updateFunc(new_values, last_sum):
        return sum(new_values) + (last_sum or 0)

    lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))
    running_counts = lines.flatMap(lambda line: line.split(&#34; &#34;))\
                          .map(lambda word: (word, 1))\
                          .updateStateByKey(updateFunc, initialRDD=initialStateRDD)

    running_counts.pprint()

    ssc.start()
    ssc.awaitTermination()
</code></pre>
<p>Lakukan simulasi seperti pada Metode 1 dengan mengirimkan Nama dan NIM di netcat ditambah beberapa kalimat lain yang ingin Anda coba eksperimen.</p>
<aside class="special"><p><strong>Catatan:</strong> Sampai sini Anda telah berhasil melakukan simulasi stateful stream processing dengan spark streaming. Silakan bisa di-screenshot hasilnya dan dibuat laporan praktikum ke repository GitHub Anda dengan nama repo atau folder (lampirkan code tersebut) <strong><code>spark-streaming/02_stateful</code></strong></p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Melakukan Transformasi di Spark Streaming" duration="20">
        <p>Selanjutnya kita akan mencoba melakukan transformasi data dengan sentimen kata sederhana seperti pada Kode 5 berikut. Silakan lakukan praktik seperti pada Metode 1 atau 2 sebelumnya.</p>
<h4 is-upgraded>Kode 5</h4>
<pre><code>from __future__ import print_function

import sys

from pyspark import SparkContext
from pyspark.streaming import StreamingContext


def print_happiest_words(rdd):
    top_list = rdd.take(5)
    print(&#34;Happiest topics in the last 5 seconds (%d total):&#34; % rdd.count())
    for tuple in top_list:
        print(&#34;%s (%d happiness)&#34; % (tuple[1], tuple[0]))

if __name__ == &#34;__main__&#34;:
    if len(sys.argv) != 3:
        print(&#34;Usage: network_wordjoinsentiments.py &lt;hostname&gt; &lt;port&gt;&#34;, file=sys.stderr)
        exit(-1)

    sc = SparkContext(appName=&#34;PythonStreamingNetworkWordJoinSentiments&#34;)
    ssc = StreamingContext(sc, 5)

    # Read in the word-sentiment list and create a static RDD from it
    word_sentiments_file_path = &#34;data/streaming/AFINN-111.txt&#34;
    word_sentiments = ssc.sparkContext.textFile(word_sentiments_file_path) \
        .map(lambda line: tuple(line.split(&#34;\t&#34;)))

    lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))

word_counts = lines.flatMap(lambda line: line.split(&#34; &#34;)) \
        .map(lambda word: (word, 1)) \
        .reduceByKey(lambda a, b: a + b)

    # Determine the words with the highest sentiment values by joining the streaming RDD
    # with the static RDD inside the transform() method and then multiplying
    # the frequency of the words by its sentiment value
    happiest_words = word_counts.transform(lambda rdd: word_sentiments.join(rdd)) \
        .map(lambda (word, tuple): (word, float(tuple[0]) * tuple[1])) \
        .map(lambda (word, happiness): (happiness, word)) \
        .transform(lambda rdd: rdd.sortByKey(False))

    happiest_words.foreachRDD(print_happiest_words)

    ssc.start()
    ssc.awaitTermination()</code></pre>
<aside class="special"><p><strong>Catatan:</strong> Sampai sini Anda telah berhasil melakukan simulasi transformasi stream processing dengan spark streaming. Silakan bisa di-screenshot hasilnya dan dibuat laporan praktikum ke repository GitHub Anda dengan nama repo atau folder (lampirkan code tersebut) <strong><code>spark-streaming/03_transformasi_word_sentiment</code></strong></p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Spark Streaming dengan Kafka dan HBase" duration="10">
        <p>On progress ...</p>
<h4 is-upgraded>Kode 6</h4>
<pre><code></code></pre>
<aside class="special"><p><strong>Catatan: </strong></p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Tugas Praktikum" duration="60">
        <ol type="1" start="1">
<li>Silakan selesaikan praktikum tersebut sesuai langkah-langkah sebelumnya, lalu laporkan hasilnya berupa link repository GitHub dengan nama <strong>spark-streaming </strong>disertai dengan screenshot hasilnya.</li>
<li>Jelaskan perbedaan spark streaming dengan metode stateless dan stateful stream processing ?</li>
<li>Jelaskan masing-masing maksud kode berikut sesuai nomor kodenya pada laporan praktikum Anda!</li>
</ol>
<table>
<tr><td colspan="1" rowspan="1"><p><strong>Nomor Kode</strong></p>
</td><td colspan="1" rowspan="1"><p><strong>Sintaks</strong></p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>1</p>
</td><td colspan="1" rowspan="1"><p><code>sys.argv, sys.stderr, StreamingContext, sc, socketTextStream, reduceByKey, lambda line, awaitTermination</code></p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>2</p>
</td><td colspan="1" rowspan="1"><p><code>nc, lk</code></p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>3</p>
</td><td colspan="1" rowspan="1"><p><code>spark-submit, master, local[*]</code></p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>4</p>
</td><td colspan="1" rowspan="1"><p><code>ssc.checkpoint, parallelize, updateStateByKey, flatMap</code></p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>5</p>
</td><td colspan="1" rowspan="1"><p><code>rdd.take(5), transform, rdd.sortByKey(False)</code></p>
</td></tr>
</table>
<ol type="1" start="4">
<li><strong>Kumpulkan link repo GitHub tersebut ke spreadsheet tugas biasanya terakhir tanggal pertemuan ke-13 berikutnya.</strong></li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Selamat!" duration="0">
        <p>Selamat Anda telah menyelesaikan Codelab ini sebagai pemahaman awal untuk memulai Apache Spark Streaming.</p>
<p>Pada codelab berikutnya, Anda akan mempelajari tentang Notebooks dan Dataflow dengan Spark dan Hadoop.</p>
<h2 is-upgraded><strong>Apa selanjutnya?</strong></h2>
<p>Silakan cek beberapa sumber belajar lainnya...</p>
<ul>
<li><a href="https://spark.apache.org/docs/latest/api/python/index.html" target="_blank">https://spark.apache.org/docs/latest/api/python/index.html</a> </li>
<li><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#spark-streaming-programming-guide" target="_blank">Spark Streaming Programming Guide</a></li>
</ul>
<h2 is-upgraded><strong>Referensi</strong></h2>
<ul>
<li><a href="https://drive.google.com/file/d/14EKNiLnJhuXw9Y6bqv2mxnCtyfRO6N0p/view?usp=sharing" target="_blank">Venkat Ankam, (2016). Big Data Analytics. Packt Publishing</a>.</li>
</ul>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/claat-public/native-shim.js"></script>
  <script src="https://storage.googleapis.com/claat-public/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/claat-public/prettify.js"></script>
  <script src="https://storage.googleapis.com/claat-public/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
